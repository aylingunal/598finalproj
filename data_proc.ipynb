{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import httpx\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data read in \n",
    "df = pd.read_csv('Contrastive.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the image files themselves (the original ZIP FILE IS 25G so we'll scrape a small subset lol)\n",
    "\n",
    "base_url = \"https://uploads8.wikiart.org/images/\"\n",
    "stopper = 0\n",
    "for item in df.iterrows():\n",
    "  item_name_spl = item[1]['painting'].split('_')\n",
    "  painter = item_name_spl[0]\n",
    "  painting = item_name_spl[1]\n",
    "  scrape_url = base_url + painter + '/' + painting + '.jpg!Large.jpg'\n",
    "\n",
    "  with open('imgs/'+item[1]['painting']+'.jpeg','wb') as outf:\n",
    "    img = httpx.get(scrape_url)\n",
    "    outf.write(img.content)\n",
    "  \n",
    "  if stopper == 25:\n",
    "    break\n",
    "  stopper += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dataset class\n",
    "class ArtemisDataset(Dataset):\n",
    "    def __init__(self, processor, labels_fname=\"Contrastive.csv\", img_dirname=\"imgs/\"):\n",
    "        self.img_labels = pd.read_csv(labels_fname).iloc[:24]\n",
    "        self.img_dirname = img_dirname\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        painting_name = self.img_labels.iloc[idx]['painting'] + '.jpeg'\n",
    "        img_path = os.path.join(self.img_dirname, painting_name)\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx]['utterance']\n",
    "        emotion = self.img_labels.iloc[idx]['emotion']\n",
    "\n",
    "        encoding = self.processor(image, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        # remove batch dimension\n",
    "        encoding = {k: v.squeeze() for k, v in encoding.items()}\n",
    "        encoding[\"input_ids\"] = self.processor.tokenizer(label)[\"input_ids\"]\n",
    "        encoding[\"attention_mask\"] = self.processor.tokenizer(label)[\"attention_mask\"]\n",
    "        encoding[\"emotion\"] = emotion\n",
    "  \n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"ybelkada/blip2-opt-2.7b-fp16-sharded\", device_map=\"auto\", load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lora for fting\n",
    "from peft import LoraConfig, get_peft_model\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ArtemisDataset(processor=processor)\n",
    "#train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=1)#, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tuning (currently buggy -- image to be converted to a PIL image contains values outside the range [0, 1],)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model.train()\n",
    "for epoch in range(2):\n",
    "  try: # this one is bc some images aren't jpeg (?)\n",
    "    for idx, item in enumerate(train_dataset):\n",
    "      print('hello')\n",
    "      try: # this one for debugging what the problem w/ model()\n",
    "        inputs = processor(images=item[\"pixel_values\"], text=item[\"input_ids\"], return_tensors=\"pt\").to(device, torch.float16)\n",
    "        outputs = model(**inputs)\n",
    "      except Exception as e:\n",
    "        print(e)\n",
    "      loss = outputs.loss\n",
    "      print('loss: ',loss.item())\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "  except:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
